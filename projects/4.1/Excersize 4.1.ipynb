{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605e6b42-6cf7-4db6-a70b-a99bd268a02b",
   "metadata": {},
   "source": [
    "# Section 4: Introduction to Random Forest\n",
    "\n",
    "## Exercise 4.1: Bagging vs Random Forest\n",
    "\n",
    "**Objective:**  \n",
    "Compare the performance of a bagging model and a random forest model on a classification problem.\n",
    "\n",
    "**Instructions:**\n",
    "1. Load a classification dataset (e.g., the Wine dataset).\n",
    "2. Train a bagging model and a random forest model on the dataset.\n",
    "3. Compare their performances in terms of accuracy, precision, recall, and computational efficiency.\n",
    "4. Discuss the impact of feature randomization in Random Forest on model performance.\n",
    "\n",
    "**Deliverables:**\n",
    "- Jupyter notebook or Python script with code and comments.\n",
    "- Performance comparison table for both models (accuracy, precision, recall).\n",
    "- Discussion of the differences in performance and impact of feature randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f311138d-28d5-4189-82ac-ac75323a53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0cbf7c-1304-4ef1-a52b-74688a475908",
   "metadata": {},
   "source": [
    "This code snippet loads the wine dataset using the load_wine function from _sklearn.datasets_, with the _as_frame=True_ parameter to return the data as a pandas DataFrame. The dataset consists of features (stored in _wine.data_) and corresponding target labels (stored in _wine.target_). The next step splits the data into training and testing sets using the _train_test_split_ function from _sklearn.model_selection_. It separates the feature data X and the target labels y into training and testing subsets, where 80% of the data is used for training and 20% for testing. The _random_state=42_ ensures that the split is reproducible, meaning the same split will occur every time the code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2292d78-844b-41ed-8f6a-12cb816fae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine(as_frame=True)\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5c471-4ebc-42d7-a058-f474dc8f8595",
   "metadata": {},
   "source": [
    "Two ensemble learning classifiers are initialized and trained on the wine dataset. The first classifier, _bagging_clf_, is a BaggingClassifier, which is an ensemble method that creates multiple models (in this case, 100) by training on different random subsets of the training data, and then combines their predictions. The second classifier, _rf_clf_, is a _RandomForestClassifier_, a specific type of bagging method that constructs multiple decision trees and aggregates their results to make predictions. Both classifiers are trained on the training data (_X_train_ and _y_train_) using the _fit()_ method. After training, predictions are made on the test data (_X_test_) for both classifiers. The predicted results are stored in _y_pred_bagging_ for the Bagging classifier and _y_pred_rf_ for the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c26b853f-6268-44ec-8e8a-3d34603e8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_clf = BaggingClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "y_pred_rf = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85d9a1-5ccc-450a-ba9c-9dae38a17740",
   "metadata": {},
   "source": [
    "We create a pandas DataFrame to compare the performance of the two trained classifiers—Bagging and Random Forest—on the test set. The DataFrame is constructed with the following columns: 'Model', 'Accuracy', 'Precision', and 'Recall'. For each model, the accuracy, precision, and recall scores are calculated using the respective functions from sklearn.metrics. The accuracy score measures the overall proportion of correct predictions, while precision and recall are calculated with a 'weighted' average, which accounts for class imbalances by weighting each class's performance by its support (the number of true instances for each class). The resulting DataFrame allows for an easy comparison of the two models' performance across these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6024ed17-e0ca-4beb-b86e-648c6c5104b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.974074</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision    Recall\n",
       "0        Bagging  0.972222   0.974074  0.972222\n",
       "1  Random Forest  1.000000   1.000000  1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'Model': ['Bagging', 'Random Forest'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_bagging), \n",
    "        accuracy_score(y_test, y_pred_rf)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_bagging, average='weighted'), \n",
    "        precision_score(y_test, y_pred_rf, average='weighted')\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_bagging, average='weighted'), \n",
    "        recall_score(y_test, y_pred_rf, average='weighted')\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffef0f6-6f41-4d26-9fa5-7ff10ca8f944",
   "metadata": {},
   "source": [
    "The comparison between Bagging and Random Forest models reveals notable differences in performance, with Random Forest outperforming Bagging across all evaluation metrics (accuracy, precision, and recall). Bagging achieved an accuracy of 97.22%, precision of 97.41%, and recall of 97.22% (these same results did not vary between several executions of the code), whereas Random Forest achieved perfect scores of 100% across all metrics. The key difference between these two models lies in the **feature randomization technique** employed by Random Forest. While Bagging simply creates multiple bootstrap samples of the data and trains individual models independently, Random Forest introduces additional randomness by selecting random subsets of features for each decision tree. This feature randomization helps reduce the correlation between trees, leading to a more diverse set of models, which can enhance generalization and prevent overfitting. As a result, Random Forest tends to provide better performance, especially in complex datasets, by capturing more diverse patterns and reducing the risk of model bias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
