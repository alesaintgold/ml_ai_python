\babel@toc {english}{}\relax 
\contentsline {chapter}{\numberline {1}Decision Trees}{2}{chapter.1}%
\contentsline {section}{\numberline {1.1}Decision trees for classification}{2}{section.1.1}%
\contentsline {paragraph}{Part A: Classification using trees}{2}{section*.2}%
\contentsline {subsection}{\numberline {1.1.1}Interpretable Models}{3}{subsection.1.1.1}%
\contentsline {paragraph}{Geometry of Flow Charts}{3}{section*.3}%
\contentsline {paragraph}{Prediction}{3}{section*.4}%
\contentsline {subsection}{\numberline {1.1.2}Classification Error}{4}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Gini index}{5}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Information Theory}{5}{subsection.1.1.4}%
\contentsline {paragraph}{What is entropy?}{5}{section*.5}%
\contentsline {subsection}{\numberline {1.1.5}Entropy}{6}{subsection.1.1.5}%
\contentsline {paragraph}{Note:}{6}{section*.6}%
\contentsline {subsection}{\numberline {1.1.6}Comparison of Criteria}{6}{subsection.1.1.6}%
\contentsline {section}{\numberline {1.2}Stopping Conditions}{7}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Common Stopping Conditions}{7}{subsection.1.2.1}%
\contentsline {paragraph}{1. Do not split a region if all instances in the region belong to the same class.}{8}{section*.7}%
\contentsline {paragraph}{2. Do not split a region if it would cause the number of instances in any sub-region to go below a pre-defined threshold (\texttt {min\_samples\_leaf}).}{8}{section*.8}%
\contentsline {paragraph}{3. Do not split a region if it would cause the total number of leaves in the tree to exceed a pre-defined threshold (\texttt {max\_leaf\_nodes}).}{8}{section*.9}%
\contentsline {paragraph}{4. Do not split if the gain is less than some pre-defined threshold (\texttt {min\_impurity\_decrease}).}{8}{section*.10}%
\contentsline {subsection}{\numberline {1.2.2}Depth-first and Best-first}{8}{subsection.1.2.2}%
\contentsline {paragraph}{Depth-first Growth}{9}{section*.11}%
\contentsline {paragraph}{Best-first Growth}{9}{section*.12}%
\contentsline {paragraph}{Depth-first vs. Best-first}{10}{section*.13}%
\contentsline {subsection}{\numberline {1.2.3}Variance vs. Bias}{10}{subsection.1.2.3}%
\contentsline {paragraph}{Cross-Validation is the key}{11}{section*.15}%
\contentsline {chapter}{\numberline {2}Decision trees II}{12}{chapter.2}%
\contentsline {section}{\numberline {2.1}Decision trees in regression}{12}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Splitting Criteria}{12}{subsection.2.1.1}%
\contentsline {paragraph}{}{12}{section*.16}%
\contentsline {paragraph}{}{12}{section*.17}%
\contentsline {paragraph}{}{12}{section*.18}%
\contentsline {subsection}{\numberline {2.1.2}Stopping Conditions}{13}{subsection.2.1.2}%
\contentsline {paragraph}{}{13}{section*.19}%
\contentsline {subsection}{\numberline {2.1.3}Prediction with regression trees}{13}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}Handling Numerical and Categorical Attributes}{13}{subsection.2.1.4}%
\contentsline {paragraph}{}{14}{section*.20}%
\contentsline {paragraph}{}{14}{section*.21}%
\contentsline {section}{\numberline {2.2}Pruning}{15}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Example: Master's application}{15}{subsection.2.2.1}%
\contentsline {paragraph}{}{15}{section*.22}%
\contentsline {subsection}{\numberline {2.2.2}Pruning techniques}{15}{subsection.2.2.2}%
\contentsline {paragraph}{}{15}{section*.23}%
\contentsline {paragraph}{}{15}{section*.24}%
\contentsline {paragraph}{}{16}{section*.25}%
\contentsline {paragraph}{}{17}{section*.26}%
\contentsline {subsection}{\numberline {2.2.3}Evaluating Pruning Options}{17}{subsection.2.2.3}%
\contentsline {paragraph}{}{17}{section*.27}%
\contentsline {paragraph}{}{18}{section*.28}%
\contentsline {subsection}{\numberline {2.2.4}Choosing the Optimal Hyperparameter}{18}{subsection.2.2.4}%
\contentsline {paragraph}{}{18}{section*.29}%
\contentsline {chapter}{\numberline {3}Bagging}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}Bagging}{19}{section.3.1}%
\contentsline {subsection}{\numberline {3.1.1}Overfitting}{19}{subsection.3.1.1}%
\contentsline {paragraph}{}{19}{section*.30}%
\contentsline {paragraph}{}{19}{section*.31}%
\contentsline {paragraph}{}{20}{section*.32}%
\contentsline {paragraph}{}{20}{section*.33}%
\contentsline {paragraph}{}{20}{section*.34}%
\contentsline {subsection}{\numberline {3.1.2}Ensemble Learning}{20}{subsection.3.1.2}%
\contentsline {paragraph}{Advantages}{21}{section*.35}%
\contentsline {paragraph}{Types of Ensemble methods}{21}{section*.36}%
\contentsline {subsection}{\numberline {3.1.3}Bootstrapping}{21}{subsection.3.1.3}%
\contentsline {paragraph}{Motivation:}{21}{section*.37}%
\contentsline {paragraph}{}{21}{section*.38}%
\contentsline {paragraph}{}{21}{section*.39}%
\contentsline {subsection}{\numberline {3.1.4}Bagging}{21}{subsection.3.1.4}%
\contentsline {paragraph}{Advantages of Bagging}{22}{section*.40}%
\contentsline {paragraph}{Bagging Types}{22}{section*.41}%
\contentsline {paragraph}{Drawbacks of Bagging}{22}{section*.42}%
\contentsline {subsection}{\numberline {3.1.5}Decision Tree vs Bagging}{23}{subsection.3.1.5}%
\contentsline {paragraph}{}{23}{section*.43}%
\contentsline {section}{\numberline {3.2}Out-of-bag error}{23}{section.3.2}%
\contentsline {paragraph}{}{23}{section*.44}%
\contentsline {paragraph}{Uses of OOB}{23}{section*.45}%
\contentsline {subsection}{\numberline {3.2.1}OOB Steps}{23}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Benefits and Drawbacks}{24}{subsection.3.2.2}%
\contentsline {paragraph}{Why use OOB error?}{24}{section*.46}%
\contentsline {paragraph}{Bagging drawbacks}{25}{section*.47}%
\contentsline {paragraph}{Improving Bagging}{25}{section*.48}%
\contentsline {chapter}{\numberline {4}Random Forest}{26}{chapter.4}%
\contentsline {paragraph}{Why Random Forests?}{26}{section*.49}%
\contentsline {section}{\numberline {4.1}Introduction to Random forest}{26}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}What is a Random Forest?}{27}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Tuning Random Forests}{28}{subsection.4.1.2}%
\contentsline {subsection}{\numberline {4.1.3}Variable Importance for Random Forest}{28}{subsection.4.1.3}%
\contentsline {paragraph}{}{28}{section*.50}%
\contentsline {subsection}{\numberline {4.1.4}Mean Decrease in Impurity (MDI)}{29}{subsection.4.1.4}%
\contentsline {paragraph}{Application of the algorithm}{29}{section*.51}%
\contentsline {paragraph}{}{30}{section*.52}%
\contentsline {subsection}{\numberline {4.1.5}Permutation Importance}{30}{subsection.4.1.5}%
\contentsline {subsection}{\numberline {4.1.6}MDI vs Permutation Importance}{31}{subsection.4.1.6}%
\contentsline {paragraph}{Advantages and drawbacks}{31}{section*.53}%
\contentsline {paragraph}{}{31}{section*.54}%
\contentsline {paragraph}{}{31}{section*.55}%
\contentsline {subsection}{\numberline {4.1.7}Variable Importance for Random Forest vs Bagging}{31}{subsection.4.1.7}%
\contentsline {section}{\numberline {4.2}Missing data}{32}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Motivation}{32}{subsection.4.2.1}%
\contentsline {paragraph}{Comparison}{33}{section*.59}%
